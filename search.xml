<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>APS</title>
      <link href="/2023/082810540.html"/>
      <url>/2023/082810540.html</url>
      
        <content type="html"><![CDATA[<h1id="paper-notes-aps-active-pretraining-with-successor-features">Papernotes: APS: Active Pretraining with Successor Features</h1><h2 id="background--motivation">Background &amp; Motivation</h2><p>Unsupervised pretraining RL enables data-efficient adaptation todownstream tasks by reward-free pretraining. Present methods mainly fallin two categories that design intrinsic reward for agents duringpretraining.</p><p>The first category is based on maximizing the mutual informationbetween task variables (<span class="math inline">\(p(z)\)</span>) andtheir behavior in terms of state visitation (<spanclass="math inline">\(p(s)\)</span>) to encourage learningdistinguishable task conditioned behaviors. VISR (Hansen et al., 2020)objective: <span class="math inline">\(\max H(z) − H(s|z)\)</span>, VISRproposes a successor features based variational approximation tomaximize a variational lower bound of the intractable conditionalentropy <span class="math inline">\(−H(s|z)\)</span>. VISR proveseffective but lacks exploration.</p><p>Another category is based on maximizing the entropy of the statesinduced by the policy <span class="math inline">\(\max H(s)\)</span>.APT by Liu &amp; Abbeel (2021) show maximizing a particle-based entropyin a lower dimensional abstraction space can boost data efficiency andasymptotic performance. But APT is task agnostic and does not adaptefficiently to new task.</p><p>APS optimizes the mutual information objective <spanclass="math inline">\(I(s;z)=H(s)-H(s|z)\)</span>. It uses APT tooptimize the state entropy term and optimizes a lower bound of theconditional entropy term with successor features.</p><p>Successor features: Assume for a set of tasks $r(s,a,s')=(s,a,s')^w $where <span class="math inline">\(w\)</span> is the task vector. Then<spanclass="math inline">\(Q(s,a)=\mathbb{E}_{s_t=s,a_t=a}\left[\sum_{l=0}^\infty\gamma^l\phi(s_{t+l},a_{t+l}, s_{t+l+1})\right]^\top w:=\psi^\pi(s,a)^\top w\)</span>, <spanclass="math inline">\(\psi^\pi(s,a)\)</span> is called successorfeature. <span class="math inline">\(\psi^\pi\)</span> can be viewed asa multi-dimensional value function that can adapt to multiple tasks.</p><h2 id="method">Method</h2><p>VISR objective is <span class="math inline">\(J_{\rmVISR}^\theta=-\mathbb{E}_{s,z}[\log q(z|s)]\)</span> where <spanclass="math inline">\(q(z|s)\)</span> is a variational approximation ofthe true distribution, trained through maximizing the log-likelihood ofsamples. VISR also restricts <span class="math inline">\(z\equivw\)</span> and <span class="math inline">\(||\phi(s)||_2=1\)</span>, andparameterizes the discriminator <spanclass="math inline">\(q(z|s)\)</span> as the Von Mises-Fisherdistribution with a scale parameter of 1: intrinsic reward <spanclass="math inline">\(r_{\rm VISR}(s,a,s&#39;)=\log q(w|s)=\phi(s)^\topw\)</span>.</p><p>APT estimates latent state entropy using a partical-based estimator<span class="math inline">\(J_{\rm APT}^\theta=\hat{H}_{\rmAPT}(h)=\sum_{i=1}^n \log \left(1+\frac{1}{k}\sum_{h^j_i\inN_k(h_i)}||h_i-h_i^j||^{n_h}_{n_h}\right)\)</span>, where <spanclass="math inline">\(N_k\)</span> denotes the <spanclass="math inline">\(k\)</span> nearest neightbors. Intrinsic reward<span class="math inline">\(r_{\rm APT}(s,a,s&#39;)= \log\left(1+\frac{1}{k}\sum_{h^j\inN_k(h)}||h-h_i||^{n_h}_{n_h}\right)\)</span>, <spanclass="math inline">\(h=f_\theta(s&#39;)\)</span>.</p><p>APS optimizes the other decomposition of mutual information <spanclass="math inline">\(I(s;z)=H(s)-H(s|z)\)</span>. It estimates theconditional entropy by a variational lower bound <spanclass="math inline">\(q(s|z)\)</span>:</p><p><span class="math display">\[\begin{align*}-H(s|z)&amp;=\sum_{s,z}p(s,z)\log p(s|z)\\ &amp;=\sum_{s,z}p(s,z)\logq(s|z)+\sum_zp(z)D_{KL}(p(\cdot|z)||q(\cdot|z))\\\&amp;\geq\mathbb{E}_{s,z}[\log q(s|z)].\end{align*}\]</span></p><p>By restricting <span class="math inline">\(z\equiv w\)</span> withthe Von Mises-Fisher distribution, similarly to VISR, we have <spanclass="math inline">\(r^{\rm exploitation}_{\rm APS}(s,a,s&#39;)=\logq(s|w)=\phi(s)^\top w\)</span>. The authors let the encoders <spanclass="math inline">\(f\)</span> and <spanclass="math inline">\(w\)</span> share weights, so <spanclass="math inline">\(r^{\rm exploration}_{\rm APS}(s,a,s&#39;)=\log\left(1+\frac{1}{k}\sum_{h^j\inN_k(h)}||h-h_i||^{n_h}_{n_h}\right)\)</span>, <spanclass="math inline">\(h=\phi(s&#39;)\)</span>.</p><p>Training of the encoder <span class="math inline">\(\phi\)</span>:loss <span class="math inline">\(L=-\mathbb{E}_{s,w}[\logq(s|w)]=-\mathbb{E}_{s,w}[\phi(s)^\top w]\)</span>.</p><p>The output layer of <span class="math inline">\(\phi\)</span> is L2normalized, task vector <span class="math inline">\(w\)</span> israndomly sampled from a uniform distribution over the unit circle.</p><p><img src="image/APS/1693318301003.png" alt="1693318301003" /></p><p>When testing the pre-trained successor features <spanclass="math inline">\(\psi\)</span>, we need to find task vector <spanclass="math inline">\(w\)</span> from the rewards. To do so, we rollout10 episodes (or 40K steps, whichever comes first) with the trained APS,each conditioned on a task vector chosen uniformly on a 5-dimensionalsphere. From these initial episodes, we combine the data across allepisodes and solve the linear regression problem. Then we fine-tune thepretrained model for 60K steps with the inferred task vector, and theaverage returns are compared.</p><blockquote><p>An intuition: different task variables represent different tasks withidentical dynamics but different distributions of rewards? VISR-likemethod basically pretrains the network on multiple tasks.</p><p>An implication of <span class="math inline">\(z\equiv w\)</span> isthat a skill is correspondent to an underlying reward distribution. Whatabout other interpretations of a skill? What is the relation betweenskill and goal conditioning?</p></blockquote><h2 id="results">Results</h2><p><img src="image/APS/1693320333875.png" alt="1693320333875" /></p><p>Table 3 shows fine-tuning and sharing encoder boost performance.</p><p><img src="image/APS/1693320399583.png" alt="1693320399583" /></p><p>Fig. 5 shows that "entropy maximization and variational successorfeatures improves each other in a nontrivial way, and both are importantto the performance gain of APS".</p><p>APS achieves SOTA on 11 of 26 Atari games (6 for APT, 3 for VISR, 4for SPR, and 2 for SimPLe) and exhibits superhuman performance on 8 of26 games. SOTA on aggregate metrics (mean/median human normalizedscore).</p><blockquote><p>By the way, human normalized score is (agent score - randomscore)/(human score - random score).</p></blockquote><h2 id="comments">Comments</h2><p>Are there some links between <spanclass="math inline">\(\psi\)</span> and <spanclass="math inline">\(\phi\)</span> ? <spanclass="math inline">\(\phi\)</span> seems to map states to the taskvariable space, and VISR reward is probably "imagining" a task withreward, but it's <span class="math inline">\(\phi^\top w\)</span>instead of <span class="math inline">\(\psi^\top w\)</span>.</p><p>Can we choose something other than the Von Mises-Fisherdistribution?</p><blockquote><p>CrossValidated: The von Mises-Fisher distribution is a distributionon the surface of a sphere. To make it as easy to visualize, think of acircle. The vM-F distribution has two parameters: the mean direction inwhich points are distributed on the circle, and how concentrated theyare around the point on the circle in that mean direction.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised pretraining RL </tag>
            
            <tag> Sucessor feature </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPR</title>
      <link href="/2023/082560493.html"/>
      <url>/2023/082560493.html</url>
      
        <content type="html"><![CDATA[<h1id="paper-notes-data-efficient-reinforcement-learning-with-self-predictive-representations-spr">Papernotes: Data-Efficient Reinforcement Learning with Self-PredictiveRepresentations (SPR)</h1><p><a href="http://arxiv.org/abs/2007.05929"class="uri">http://arxiv.org/abs/2007.05929</a></p><p>Published at ICLR 2021.</p><h2 id="motivation">Motivation</h2><p>Data efficiency in Q-learning (for playing Atari games) is a problem.Learning a representation of states can boost sample efficiency and helpgeneralization. Forcing representations to be temporally predictive andconsistent and using data augmentation (pixel-based setting) train abetter representation.</p><h2 id="method">Method</h2><p><img src="image/SPR/1692976244789.png" alt="1692976244789" /></p><p><img src="image/SPR/1692977180091.png" alt="1692977180091" /></p><p>Highlights:</p><ul><li><p>Use separate online encoder and target encoder (updated throughEMA but no gradient step)</p><ul><li>EMA: <span class="math inline">\(\theta_m\gets \tau \theta_m +(1-\tau)\theta_o\)</span></li></ul></li><li><p>Self-supervised prediction through a learned transitionmodel</p></li><li><p>Projection heads (why do we have to use that? and the additionalhead <span class="math inline">\(q\)</span> ?)</p></li><li><p>Cosine similarity loss</p><p><span class="math display">\[\mathcal{L}^{\rm SPR}_\theta(s_{t:t+K}, a_{t:t+K})=-\sum_{k=1}^K\frac{\tilde{y}_{t+k}^\top}{||\tilde{y}_{t+k}||_2}\frac{\hat{y}_{t+k}}{||\hat{y}_{t+k}||_2}\]</span></p></li></ul><p>Note:</p><ul><li>The authors' implementation includes <spanclass="math inline">\(g_o\)</span> in the Q-learning head.</li><li>The authors claim that when data augmentation is used, <spanclass="math inline">\(\tau\)</span> can be 0 (fully online) to dowell.</li></ul><h2 id="results">Results</h2><p><img src="image/SPR/1692977266038.png" alt="1692977266038" /></p><p>SOTA performance (using data augmentation) with limited (100k)interaction budget.</p><h2 id="questions">Questions</h2><ul><li>How to deal with stochastic dynamics?</li><li>Can we further reduce experience needed on a new task through metalearning or pretraining?</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Self-supervised RL </tag>
            
            <tag> Representation learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂项问题归档</title>
      <link href="/2023/081616452.html"/>
      <url>/2023/081616452.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-wsl启动失败">1 WSL启动失败</h1><p>问题：尝试启动wsl时失败</p><pre class="language-bash" data-language="bash"><code class="language-bash">$ wsl适用于 Linux 的 Windows 子系统实例已终止。</code></pre><p>解决：在<strong>管理员身份下</strong>运行</p><pre class="language-bash" data-language="bash"><code class="language-bash">$ net stop LxssManager$ net start LxssManager</code></pre><p>如果不是admin可能会报错 <code>发生系统错误 5。</code></p><h1 id="2-清除hexo网页缓存">2 清除Hexo网页缓存</h1><p>问题：某些时候更改 <code>_config.yaml</code> 之后更改没有应用</p><p>解决：删除 <code>db.json</code> 之后重新生成网页</p>]]></content>
      
      
      <categories>
          
          <category> misc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> misc </tag>
            
            <tag> 中文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/081316107.html"/>
      <url>/2023/081316107.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
